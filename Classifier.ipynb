{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"L-m5MuKp00Ks"},"outputs":[],"source":["### Inclusions ###\n","\n","import sys, os, pickle\n","from IPython.display import clear_output\n","from tqdm import tqdm\n","sys.path.append('/home/sj/ml/lib/python3.10/site-packages/') # *Change path as required*\n","\n","from scipy.stats import percentileofscore\n","import csv\n","\n","import numpy as np\n","import pandas as pd\n","import importlib\n","import random\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import LogNorm\n","# Plots stuff\n","import matplotlib as mpl\n","from matplotlib import patches\n","from pandas.plotting import table\n","\n","import sklearn\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split, KFold\n","from skbio import DNA, Protein\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.nn import L1Loss\n","import copy\n","from scipy.optimize import fsolve\n","import math\n","\n","A,L=20,9\n","\n","# import keras\n","# from keras.models import Sequential\n","# from keras.layers import Dense, Dropout\n","# from keras import regularizers\n","\n","def overlap_seqs(list1,list2):\n","    overlap=[]\n","    for i in range(len(list1)):\n","        if list1[i] in list2:\n","            overlap.append(list1[i])\n","    return overlap\n","\n","def flatten_list(listoflist):\n","    listoflist_fl = [];\n","    for l in range(len(listoflist)):\n","        for u in range(len(listoflist[l])):\n","            listoflist_fl.append(listoflist[l][u])\n","    return listoflist_fl\n","\n","curr_int = np.int16\n","def convert_number(seqs): # convert to numbers already aligned seqs\n","    aa = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V',  'W', 'Y','-']\n","    aadict = {aa[k]: k for k in range(len(aa))}\n","\n","    msa_num = np.array(list(map(lambda x: [aadict[y] for y in x], seqs[0:])), dtype=curr_int, order=\"c\") ### Here change ####\n","\n","    return msa_num\n","\n","def uniqueIndexes(l):\n","    seen = set()\n","    res = []\n","    for i, n in enumerate(l):\n","        if n not in seen:\n","            res.append(i)\n","            seen.add(n)\n","    return res\n","\n","def convert_letter(seqs_n): # convert to numbers already aligned seqs\n","    aa = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V',  'W', 'Y','-']\n","    aadictinv = {k: aa[k] for k in range(len(aa))}\n","    seqs=[]\n","    if type(seqs_n[0]) == curr_int:\n","        seqs.append(''.join([aadictinv[e] for e in seqs_n]))\n","    else:\n","        for t in range(len(seqs_n)):\n","            seqs.append(''.join([aadictinv[e] for e in seqs_n[t]]))\n","    return seqs\n","\n","# add some functions for independent site models\n","def loglikelihood_indip_model(fields, logZ, seqs):\n","    return fields[np.arange(len(fields)), seqs].sum(axis=1) - logZ\n","\n","def add_pseudocount(fields, n):\n","    return np.array([(f + 1/n) / np.sum((f + 1/n)) for f in fields])\n","\n","def build_model(dims, dropout_prob=0.5):\n","    assert dims[0] == A * L\n","    assert dims[-1] == 1\n","\n","    layers = [torch.nn.Flatten(), torch.nn.Linear(dims[0], dims[1])]\n","    for l in range(2, len(dims)):\n","        layers.append(torch.nn.LeakyReLU())\n","        layers.append(torch.nn.Linear(dims[l - 1], dims[l]))\n","        if l < len(dims) - 1:  # Add dropout except for the last layer\n","            layers.append(torch.nn.Dropout(p=dropout_prob))\n","    return torch.nn.Sequential(*layers)\n","\n","def getAllModels(A=20, L=9, dropout_prob=0.5):\n","    return [\n","        build_model([A * L, 1], dropout_prob), # Perceptron (Parsimonious)\n","        build_model([A * L, 2, 1], dropout_prob),\n","        build_model([A * L, 4, 1], dropout_prob),\n","        build_model([A * L, 8, 1], dropout_prob),\n","        build_model([A * L, 16, 1], dropout_prob),\n","        build_model([A * L, 32, 1], dropout_prob),\n","        build_model([A * L, 64, 1], dropout_prob),\n","        build_model([A * L, 128, 1], dropout_prob),\n","        build_model([A * L, 16, 8, 1], dropout_prob),\n","        build_model([A * L, 32, 8, 1], dropout_prob),\n","        build_model([A * L, 64, 8, 1], dropout_prob),\n","        build_model([A * L, 128, 8, 1], dropout_prob),\n","        build_model([A * L, 32, 16, 1], dropout_prob),\n","        build_model([A * L, 64, 16, 1], dropout_prob),\n","        build_model([A * L, 128, 16, 1], dropout_prob),\n","        build_model([A * L, 32, 16, 8, 1], dropout_prob),\n","        build_model([A * L, 64, 16, 8, 1], dropout_prob),\n","        build_model([A * L, 64, 32, 16, 1], dropout_prob),\n","        build_model([A * L, 128, 32, 16, 1], dropout_prob),\n","        build_model([A * L, 128, 64, 16, 1], dropout_prob),\n","        build_model([A * L, 128, 64, 32, 1], dropout_prob),\n","        build_model([A * L, 128, 64, 32, 8, 1], dropout_prob),\n","        build_model([A * L, 128, 64, 32, 16, 1], dropout_prob)]\n","\n","def getModelsBest(A=20, L=9, dropout_prob=0.5):\n","    # PRIME-Trained uses best = 6\n","    return [\n","        build_model([A * L, 1], dropout_prob), # Perceptron (Parsimonious)\n","        build_model([A * L, 64, 1], dropout_prob)]\n","\n","def getNumInputFeatures(X_train, X_test):\n","    allLetters = ''\n","    for i in X_train:\n","        allLetters= allLetters + i\n","    for i in X_test:\n","        allLetters= allLetters + i\n","    A = len(np.unique([*allLetters]))\n","    return A"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WD7XQvDx00Kz"},"outputs":[],"source":["def getPeps(HLA, isSarsCov2 = False):\n","\n","    ## Block that filters the data -  https://www.iedb.org/database_export_v3.php ##\n","    # import IEDB -  for T cell assays - and read relevant columns #\n","    filename_lab = 'tcell_full_v3.csv'\n","    iedb = pd.read_csv(filename_lab, sep=',', low_memory=False)\n","    head = iedb.columns\n","\n","    index_species = 43 # column indicating in what species the antigens are tested\n","    index_antigen = 11 # column with antigens\n","\n","    condition0 = iedb[head[146]] == 'Linear peptide'  # to be sure a peptide immunization study was used\n","    condition1 = iedb[head[145]] == 'Epitope' # to have the peptide as first immunogen, not the full protein or virus\n","    condition2 = iedb[head[144]] == 'I' # to select CD8 epitopes\n","\n","    list_hosts = list(np.unique(iedb[head[index_species]].astype(str)))\n","    list_mus = [st for st in list_hosts if 'Mus' in st]\n","    #condition3 = iedb[head[index_species]].isin(list_mus)\n","    list_homo = [st for st in list_hosts if 'Homo' in st]\n","    #condition3 = iedb[head[index_species]].isin(list_homo + list_mus)\n","    condition3 = iedb[head[index_species]].isin(list_homo) # Human only\n","\n","    # Exclude SARS-CoV-2 data for train data\n","    list_antigenNames = list(np.unique(iedb[head[23]].astype(str)))\n","    list_sarscov2 = [st for st in list_antigenNames if 'SARS-CoV2' in st or 'Severe acute respiratory syndrome coronavirus 2' in st]\n","    if isSarsCov2 == False:\n","        condition4 = ~iedb[head[23]].isin(list_sarscov2)\n","    else:\n","        condition4 = iedb[head[23]].isin(list_sarscov2)\n","\n","    iedbT = iedb[condition0 & condition1 & condition2 & condition3 & condition4]\n","\n","    listT = ['T cell CD4+','T cell CD4-'] ## exclude CD4 responders in case there was someone left\n","    conditionT = ~iedbT[head[95]].isin(listT) # note: conditionT no strictly necessary #\n","\n","    conditionP0 = iedbT[head[94]] != 'Restimulation in vitro' # Here, if the evidence comes from in vitro, exclude a restimulation step\n","    conditionP1 = iedbT[head[122]].isin(['Positive', 'Positive-High'])\n","    conditionP10 = iedbT[head[122]].isin(['Positive', 'Positive-High','Positive-Intermediate' ,'Positive-Low'])\n","    conditionN1 = iedbT[head[122]] == 'Negative'\n","\n","    iedbTN = iedbT[conditionP0 & conditionN1 & conditionT]\n","    iedbTP = iedbT[conditionP0 & conditionP1 & conditionT]\n","    iedbTP0 = iedbT[conditionP0 & conditionP10 & conditionT]\n","\n","#     list_hla = list(np.unique(iedbTN[head[141]].values))\n","#     list_hlaFn=[]\n","#     for hh in list_hla:\n","#         print(hh)\n","#         ii = iedbTN[iedbTN[head[141]] == hh]\n","#         if len(np.unique((ii[head[index_antigen]].values))) > 50:\n","#             list_hlaFn.append(hh)\n","\n","#     list_hla = list(np.unique(iedbTP[head[141]].values))\n","#     list_hlaFp=[]\n","#     for hh in list_hla:\n","#         print(hh)\n","#         ii = iedbTP[iedbTP[head[141]] == hh]\n","#         if len(np.unique((ii[head[index_antigen]].values))) > 50:\n","#             list_hlaFp.append(hh)\n","\n","#     list_hlaF0 = overlap_seqs(list_hlaFn,list_hlaFp)\n","#     list_hlaF = [f for f in list_hlaF0 if '*' in f]\n","\n","#     iedbTNa = iedbTN[iedbTN[head[141]].isin(list_hlaF)]\n","#     iedbTPa = iedbTP[iedbTP[head[141]].isin(list_hlaF)]\n","\n","    hh = HLA\n","    if hh[6] != 0:\n","        hhM = hh[:5] + hh[7]\n","    else:\n","        hhM = hh[:5] + hh[6:7]\n","\n","    iedbTNa = iedbTN[iedbTN[head[141]]==hh]\n","    iedbTPa = iedbTP[iedbTP[head[141]]==hh]\n","    iedbTP0a = iedbTP0[iedbTP0[head[141]].isin([hh,hhM])]\n","    iedbTN0a = iedbTN[iedbTN[head[141]].isin([hh,hhM])]\n","    pep_imm = list(np.unique((iedbTPa[head[index_antigen]].values)))\n","    pep_imm0 = list(np.unique((iedbTP0a[head[index_antigen]].values)))\n","    pep_immN = list(np.unique((iedbTNa[head[index_antigen]].values)))\n","    pep_immN0 = list(np.unique((iedbTN0a[head[index_antigen]].values)))\n","\n","    pep_imm9 = [pep_imm[p] for p in range(len(pep_imm)) if len(pep_imm[p]) in range_len and 'X' not in pep_imm[p] and 'l' not in pep_imm[p] and pep_imm[p] not in pep_immN0]\n","    pep_imm9N = [pep_immN[p] for p in range(len(pep_immN)) if len(pep_immN[p]) in range_len and pep_immN[p] not in pep_imm0]\n","    pep_imm9 = [pep for pep in pep_imm9 if len(pep) == 9]\n","    pep_imm9N = [pep for pep in pep_imm9N if len(pep) == 9]\n","    return list(pep_imm9), list(pep_imm9N)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IepdRdeW00K4"},"outputs":[],"source":["# Build Training Database\n","\n","range_len = [8, 9, 10, 11]\n","list_hlas = ['HLA-A*01:01', 'HLA-A*02:01', 'HLA-A*03:01', 'HLA-A*11:01', 'HLA-A*24:02', 'HLA-B*07:02', 'HLA-B*08:01', 'HLA-B*15:01', 'HLA-B*35:01', 'HLA-B*40:01'] # 10 most frequent alleles for this problem\n","#list_hlas = ['HLA-B*08:01', 'HLA-B*15:01', 'HLA-B*35:01']\n","\n","pep_dict_train = {}  # Dictionary to store peptides for each HLA allele\n","\n","pepsP_train, pepsN_train = [], []\n","\n","for HLA in list_hlas:\n","    hlaPeps = getPeps(HLA, False)\n","    hlaPepsP, hlaPepsN = hlaPeps[0], hlaPeps[1]\n","\n","    print(HLA, ':')\n","    print('Num. of positives:', len(hlaPepsP))\n","    print('Num. of negatives:', len(hlaPepsN))\n","\n","    pep_dict_train[HLA] = [hlaPepsP, hlaPepsN]  # Store positives for HLA allele in the dictionary\n","    pepsP_train.append(hlaPepsP)  # Store positives in the list\n","    pepsN_train.append(hlaPepsN)  # Store negatives in the list\n","\n","    # Check no overlap left between positives and negatives #\n","    if len(overlap_seqs(hlaPepsP, hlaPepsN)) > 0:\n","        print('Overlap between positives & negatives exists.')\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9vMjAhvd00K7"},"outputs":[],"source":["# Build Test Database\n","\n","pep_dict_test = {}  # Dictionary to store peptides for each HLA allele in the test set\n","\n","pepsP_test, pepsN_test = [], []\n","\n","for HLA in list_hlas:\n","    hlaPeps = getPeps(HLA, True)\n","    hlaPepsP, hlaPepsN = hlaPeps[0], hlaPeps[1]\n","\n","    print(HLA, ':')\n","    print('Num. of positives:', len(hlaPepsP))\n","    print('Num. of negatives:', len(hlaPepsN))\n","\n","    pep_dict_test[HLA] = [hlaPepsP, hlaPepsN]  # Store positives for HLA allele in the dictionary\n","    pepsP_test.append(hlaPepsP)  # Store positives in the list\n","    pepsN_test.append(hlaPepsN)  # Store negatives in the list\n","\n","    # Check no overlap left between positives and negatives #\n","    if len(overlap_seqs(hlaPepsP, hlaPepsN)) > 0:\n","        print('Overlap between positives & negatives exists.')\n","    print()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zPe4XCgJ00K9","outputId":"0ab4733c-be85-4b34-b7fc-cf49d253220f"},"outputs":[{"data":{"text/plain":["3236"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["count = 0\n","\n","for i in list_hlas:\n","    count += len(pep_dict_train[i][0])\n","    count += len(pep_dict_test[i][0])\n","count"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ArjSUtdF00K_","outputId":"97e77474-48d6-40c7-e77f-2fe44d2190de"},"outputs":[{"data":{"text/plain":["7094"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["count = 0\n","for i in list_hlas:\n","    count += len(pep_dict_train[i][1])\n","print(count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U7Z0QMyk00LB","outputId":"c5419d71-b20b-4bbd-d0ef-de978dc5aba2"},"outputs":[{"name":"stdout","output_type":"stream","text":["2480\n","6914\n","756\n","180\n"]}],"source":["count = 0\n","for i in list_hlas:\n","    count += len(pep_dict_train[i][0])\n","print(count)\n","\n","count = 0\n","for i in list_hlas:\n","    count += len(pep_dict_train[i][1])\n","print(count)\n","\n","count = 0\n","for i in list_hlas:\n","    count += len(pep_dict_test[i][0])\n","print(count)\n","\n","count = 0\n","for i in list_hlas:\n","    count += len(pep_dict_test[i][1])\n","print(count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sokjKivg00LD"},"outputs":[],"source":["# Save/Load IEDB Data\n","\n","range_len = [8, 9, 10, 11]\n","list_hlas = ['HLA-A*01:01', 'HLA-A*02:01', 'HLA-A*03:01', 'HLA-A*11:01', 'HLA-A*24:02', 'HLA-B*07:02', 'HLA-B*08:01', 'HLA-B*15:01', 'HLA-B*35:01', 'HLA-B*40:01']\n","\n","# # Save the variables, dictionaries, and arrays to a file\n","# with open('Data/iedb_data.pkl', 'wb') as file:  # *Change path as required*\n","#     data = {\n","#         'pep_dict_train': pep_dict_train,\n","#         'pepsP_train': pepsP_train,\n","#         'pepsN_train': pepsN_train,\n","#         'pep_dict_test': pep_dict_test,\n","#         'pepsP_test': pepsP_test,\n","#         'pepsN_test': pepsN_test\n","#     }\n","#     pickle.dump(data, file)\n","\n","# Load the variables, dictionaries, and arrays from the file\n","with open('Data/iedb_data.pkl', 'rb') as file:  # *Change path as required*\n","    data = pickle.load(file)\n","    pep_dict_train = data['pep_dict_train']\n","    pepsP_train = data['pepsP_train']\n","    pepsN_train = data['pepsN_train']\n","    pep_dict_test = data['pep_dict_test']\n","    pepsP_test = data['pepsP_test']\n","    pepsN_test = data['pepsN_test']\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O-SwZaYM00LF"},"outputs":[],"source":["def getSplitData(pepsP_train, pepsN_train, pepsP_test, pepsN_test, split=0.25):\n","\n","    hlaP_train, hlaN_train, hlaP_test, hlaN_test = [], [], [], []\n","\n","    for alleles in range(len(list_hlas)):\n","        hlaP_train.append([list_hlas[alleles]] * len(pepsP_train[alleles]))\n","        hlaN_train.append([list_hlas[alleles]] * len(pepsN_train[alleles]))\n","        hlaP_test.append([list_hlas[alleles]] * len(pepsP_test[alleles]))\n","        hlaN_test.append([list_hlas[alleles]] * len(pepsN_test[alleles]))\n","\n","    # Concatenate positive and negative peptides for each allele\n","    pepsP_train_concat = np.concatenate(pepsP_train)\n","    pepsN_train_concat = np.concatenate(pepsN_train)\n","    pepsP_test_concat = np.concatenate(pepsP_test)\n","    pepsN_test_concat = np.concatenate(pepsN_test)\n","\n","    # Concatenate HLA labels for each peptide\n","    hlaP_train_concat = np.concatenate(hlaP_train)\n","    hlaN_train_concat = np.concatenate(hlaN_train)\n","    hlaP_test_concat = np.concatenate(hlaP_test)\n","    hlaN_test_concat = np.concatenate(hlaN_test)\n","\n","    Ptest_cutoff = int(split*len(pepsP_test_concat))\n","    Ntest_cutoff = int(split*len(pepsN_test_concat))\n","\n","    # Create X_train by combining non-SARS-CoV-2 data with 25% of SARS-CoV-2 data from pepsP_test and pepsN_test\n","    X_train = np.concatenate([pepsP_train_concat, pepsN_train_concat, pepsP_test_concat[:Ptest_cutoff], pepsN_test_concat[:Ntest_cutoff]])\n","    X_test = np.concatenate([pepsP_test_concat[Ptest_cutoff:], pepsN_test_concat[Ntest_cutoff:]])\n","\n","    # Create y_train by assigning 1 to immunogenic peptides and 0 to non-immunogenic peptides\n","    y_train = np.concatenate([np.ones(len(pepsP_train_concat)), np.zeros(len(pepsN_train_concat)), np.ones(Ptest_cutoff), np.zeros(Ntest_cutoff)])\n","    y_test = np.concatenate([np.ones(len(pepsP_test_concat[Ptest_cutoff:])), np.zeros(len(pepsN_test_concat[Ntest_cutoff:]))])\n","\n","    # Create hla_train, hla_test\n","    hla_train = np.concatenate([hlaP_train_concat, hlaN_train_concat, hlaP_test_concat[:Ptest_cutoff], hlaN_test_concat[:Ntest_cutoff]])\n","    hla_test = np.concatenate([hlaP_test_concat[Ptest_cutoff:], hlaN_test_concat[Ntest_cutoff:]])\n","\n","    # Shuffle the data\n","    random_indices_train = np.random.permutation(len(X_train))\n","    random_indices_test = np.random.permutation(len(X_test))\n","\n","    X_train = X_train[random_indices_train]\n","    hla_train = hla_train[random_indices_train]\n","    y_train = y_train[random_indices_train]\n","\n","    X_test = X_test[random_indices_test]\n","    hla_test = hla_test[random_indices_test]\n","    y_test = y_test[random_indices_test]\n","\n","    return X_train, y_train, hla_train, X_test, y_test, hla_test\n","\n","def getSplitDataHLA(HLA, split=0.25):\n","    # (For single allele classifier)\n","    p_train, n_train, p_test, n_test = pep_dict_train[HLA][0], pep_dict_train[HLA][1], pep_dict_test[HLA][0], pep_dict_test[HLA][1]\n","\n","    Ptest_cutoff = int(split*len(p_test))\n","    Ntest_cutoff = int(split*len(n_test))\n","\n","    # Create X_train by combining non-sars-cov-2 data with 25% of sars-cov-2 data from pepsP_test and pepsN_test\n","    X_train = np.concatenate([p_train, n_train, p_test[:Ptest_cutoff], n_test[:Ntest_cutoff]])\n","    X_test = np.concatenate([p_test[Ptest_cutoff:], n_test[Ntest_cutoff:]])\n","\n","    # Create y_train by assigning 1 to immunogenic peptides and 0 to non-immunogenic peptides\n","    y_train = np.concatenate([np.ones(len(p_train)), np.zeros(len(n_train)), np.ones(Ptest_cutoff), np.zeros(Ntest_cutoff)])\n","    y_test = np.concatenate([np.ones(len(p_test[Ptest_cutoff:])), np.zeros(len(n_test[Ntest_cutoff:]))])\n","\n","    # Shuffle the data\n","    random_indices_train = np.random.permutation(len(X_train))\n","    random_indices_test = np.random.permutation(len(X_test))\n","    X_train = X_train[random_indices_train]\n","    y_train = y_train[random_indices_train]\n","    X_test = X_test[random_indices_test]\n","    y_test = y_test[random_indices_test]\n","\n","    return X_train, y_train, X_test, y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oI2RrIq300LJ"},"outputs":[],"source":["def getPrimeTrainData():\n","    # Load PRIME2.0 Train (prtr) Data\n","    with open('Data/prime_peps_seq.txt', 'r') as file:\n","        prtr_pep = [line.strip() for line in file]\n","    with open('Data/prime_peps_imm.txt', 'r') as file:\n","        prtr_imm = [int(line.strip()) for line in file]\n","    with open('Data/prime_peps_HLA.txt', 'r') as file:\n","        prtr_hla = ['HLA-' + line.strip()[0] + '*' + line.strip()[1:3] + ':' + line.strip()[3:5] for line in file]\n","\n","    # Filter elements to remove peptides of length != 9\n","    prtr_pep, prtr_imm, prtr_hla = zip(*[(p, i, h) for p, i, h in zip(prtr_pep, prtr_imm, prtr_hla) if len(p) == 9])\n","    X_train, y_train, hla_train = np.array(prtr_pep), np.array(prtr_imm), np.array(prtr_hla)\n","    _, _, _, X_test, y_test, hla_test = getSplitData(pepsP_train, pepsN_train, pepsP_test, pepsN_test, split=0.05)\n","\n","    return X_train, y_train, hla_train, X_test, y_test, hla_test\n","\n","import numpy as np\n","\n","def getPrimeTrainDataSplit(HLA):\n","    # Load PRIME2.0 Train (prtr) Data\n","    with open('Data/prime_peps_seq.txt', 'r') as file:\n","        prtr_pep = [line.strip() for line in file]\n","    with open('Data/prime_peps_imm.txt', 'r') as file:\n","        prtr_imm = [int(line.strip()) for line in file]\n","    with open('Data/prime_peps_HLA.txt', 'r') as file:\n","        prtr_hla = ['HLA-' + line.strip(s)[0] + '*' + line.strip()[1:3] + ':' + line.strip()[3:5] for line in file]\n","\n","    # Filter elements to remove peptides of length != 9\n","    prtr_pep, prtr_imm, prtr_hla = zip(*[(p, i, h) for p, i, h in zip(prtr_pep, prtr_imm, prtr_hla) if len(p) == 9])\n","    # Convert to NumPy arrays\n","    X_train, y_train, hla_train = np.array(prtr_pep), np.array(prtr_imm), np.array(prtr_hla)\n","    # Filter X_train and y_train based on the HLA indices\n","    indices = np.where(hla_train == HLA)[0]\n","    X_train, y_train = X_train[indices], y_train[indices]\n","    _, _, X_test, y_test = getSplitDataHLA(HLA, split=0.05)\n","    return X_train, y_train, X_test, y_test\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sUphErQc00LJ"},"outputs":[],"source":["def getAgnosticData(trainSplit=0.8):\n","    \"\"\"\n","    Combine train and test data from peptide dictionaries into a virus-agnostic dataset.\n","    Split the combined data into train and test sets based on the specified train split ratio.\n","    Return the train and test datasets along with corresponding HLA lists.\n","\n","    Args:\n","        trainSplit (float): The ratio of data to be allocated for training (default: 0.8)\n","\n","    Returns:\n","        tuple: X_train (list), y_train (list), hla_train (list), X_test (list), y_test (list), hla_test (list)\n","    \"\"\"\n","    X_train, y_train, hla_train, X_test, y_test, hla_test = [], [], [], [], [], []\n","\n","    for hla in list_hlas:\n","        # Combine all X and y values for the current HLA\n","        all_X = pep_dict_train[hla][0] + pep_dict_train[hla][1] + pep_dict_test[hla][0] + pep_dict_test[hla][1]\n","        all_y = len(pep_dict_train[hla][0]) * [1] + len(pep_dict_train[hla][1]) * [0] + len(pep_dict_test[hla][0]) * [1] + len(pep_dict_test[hla][1]) * [0]\n","\n","        # Shuffle the combined X and y values\n","        combined = list(zip(all_X, all_y))\n","        random.shuffle(combined)\n","        all_X, all_y = zip(*combined)\n","\n","        # Calculate the split index based on the train split ratio\n","        split_index = int(len(all_X) * trainSplit)\n","\n","        # Append the train and test data for the current HLA to the respective lists\n","        X_train.extend(all_X[:split_index])\n","        y_train.extend(all_y[:split_index])\n","        hla_train.extend(len(all_X[:split_index]) * [hla])\n","        X_test.extend(all_X[split_index:])\n","        y_test.extend(all_y[split_index:])\n","        hla_test.extend(len(all_X[split_index:]) * [hla])\n","\n","    # Shuffle the combined train and test data\n","    combined = list(zip(X_train, y_train, hla_train))\n","    random.shuffle(combined)\n","    X_train, y_train, hla_train = zip(*combined)\n","\n","    combined = list(zip(X_test, y_test, hla_test))\n","    random.shuffle(combined)\n","    X_test, y_test, hla_test = zip(*combined)\n","\n","    return np.array(X_train), np.array(y_train), np.array(hla_train), np.array(X_test), np.array(y_test), np.array(hla_test)\n","\n","def getAgnosticDataHLA(hla, trainSplit=0.8):\n","    \"\"\"\n","    Combine train and test data from the peptide dictionaries for a specific HLA allele into a virus-agnostic dataset.\n","    Split the combined data into train and test sets based on the specified train split ratio.\n","    Return the train and test datasets.\n","\n","    Args:\n","        hla (str): The HLA allele for which to retrieve the data.\n","        trainSplit (float): The ratio of data to be allocated for training (default: 0.8)\n","\n","    Returns:\n","        tuple: X_train (list), y_train (list), X_test (list), y_test (list)\n","    \"\"\"\n","    # Combine all X and y values for the HLA\n","    all_X = pep_dict_train[hla][0] + pep_dict_train[hla][1] + pep_dict_test[hla][0] + pep_dict_test[hla][1]\n","    all_y = len(pep_dict_train[hla][0]) * [1] + len(pep_dict_train[hla][1]) * [0] + len(pep_dict_test[hla][0]) * [1] + len(pep_dict_test[hla][1]) * [0]\n","\n","    # Shuffle the combined X and y values\n","    combined = list(zip(all_X, all_y))\n","    random.shuffle(combined)\n","    all_X, all_y = zip(*combined)\n","\n","    # Calculate the split index based on the train split ratio\n","    split_index = int(len(all_X) * trainSplit)\n","\n","    # Split the data into train and test sets\n","    X_train = np.array(all_X[:split_index])\n","    y_train = np.array(all_y[:split_index])\n","    X_test = np.array(all_X[split_index:])\n","    y_test = np.array(all_y[split_index:])\n","\n","    return X_train, y_train, X_test, y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BgB_-yQw00LM"},"outputs":[],"source":["# Classifier Functions & Definitions\n","\n","def classifier_auc(model_idx, X_train, y_train, X_test, y_test, epochs=50, weight_decay=0.6):\n","    # Unweighted\n","    # Define the hyperparameters and early stopping variables\n","    input_size = A  # Adjust according to the number of features in your input data\n","    learning_rate = 0.0005\n","    batch_size = 16\n","    random.seed = 42\n","    num_epochs = epochs\n","    patience = int(num_epochs**0.5)\n","    early_stopping_counter = 0\n","    best_loss = float('inf')\n","\n","    # Create the model\n","    model = copy.deepcopy(classifiers[model_idx])\n","\n","    # Define the loss function and optimizer\n","    optimizer = torch.optim.AdamW(model.parameters(), weight_decay=weight_decay)\n","    loss_function = torch.nn.BCEWithLogitsLoss()\n","\n","    # Convert the data to PyTorch tensors\n","    X_train_tensor = torch.nn.functional.one_hot(torch.LongTensor(convert_number(X_train)), num_classes=A).type(torch.FloatTensor)\n","    y_train_tensor = torch.Tensor(y_train)\n","    X_test_tensor = torch.nn.functional.one_hot(torch.LongTensor(convert_number(X_test)), num_classes=A).type(torch.FloatTensor)\n","    y_test_tensor = torch.Tensor(y_test)\n","    train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size)\n","\n","    # Reset model parameters\n","    for layer in model.children():\n","        if hasattr(layer, 'reset_parameters'):\n","            layer.reset_parameters()\n","\n","    # Train the model\n","    for epoch in range(num_epochs):\n","        for batch_idx, (batch, y) in enumerate(train_loader):\n","            optimizer.zero_grad()\n","            y_pred = model(batch)\n","            loss = loss_function(y_pred, y.unsqueeze(1))\n","            loss.backward()\n","            optimizer.step()\n","        # Calculate validation loss\n","        with torch.no_grad():\n","            y_pred_val = model(X_test_tensor)\n","            loss_val = loss_function(y_pred_val, y_test_tensor.unsqueeze(1))\n","\n","        # Check for early stopping\n","        if loss_val < best_loss:\n","            best_loss = loss_val\n","            early_stopping_counter = 0\n","        else:\n","            early_stopping_counter += 1\n","            if early_stopping_counter >= patience:\n","                break\n","\n","    # Calculate ROC curve and AUC\n","    fpr_train, tpr_train, thresholds_train = sklearn.metrics.roc_curve(\n","        np.concatenate((np.zeros(len(X_train_tensor[y_train_tensor == 0])) + 0,\n","                        np.zeros(len(X_train_tensor[y_train_tensor == 1])) + 1), axis=0),\n","        np.concatenate((model(X_train_tensor[y_train_tensor == 0]).detach().numpy(),\n","                        model(X_train_tensor[y_train_tensor == 1]).detach().numpy()), axis=0)\n","    )\n","    fpr_tests, tpr_tests, thresholds_tests = sklearn.metrics.roc_curve(\n","        np.concatenate((np.zeros(len(X_test_tensor[y_test_tensor == 0])) + 0,\n","                        np.zeros(len(X_test_tensor[y_test_tensor == 1])) + 1), axis=0),\n","        np.concatenate((model(X_test_tensor[y_test_tensor == 0]).detach().numpy(),\n","                        model(X_test_tensor[y_test_tensor == 1]).detach().numpy()), axis=0)\n","    )\n","    auc_train = sklearn.metrics.auc(fpr_train, tpr_train)\n","    auc_tests = sklearn.metrics.auc(fpr_tests, tpr_tests)\n","    return {'model_idx': model_idx, 'weight_decay': weight_decay, 'auc_train': auc_train, 'auc_tests': auc_tests}\n","\n","def do_auc(model_idx, weight_decay=0.6, epochs=50, num_folds=5):\n","    fold_results = []\n","    kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n","    for train_index, val_index in kf.split(X_train):\n","        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n","        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n","        auc = classifier_auc(model_idx, X_train_fold, y_train_fold, X_val_fold, y_val_fold, weight_decay=weight_decay, epochs=epochs)\n","        fold_results.append(auc)\n","    avg_auc_train = np.mean([fold['auc_train'] for fold in fold_results])\n","    avg_auc_tests = np.mean([fold['auc_tests'] for fold in fold_results])\n","    std_auc_train = np.std([fold['auc_train'] for fold in fold_results])\n","    std_auc_tests = np.std([fold['auc_tests'] for fold in fold_results])\n","    return {'model_idx': model_idx, 'weight_decay': weight_decay, 'auc_train': avg_auc_train, 'auc_tests': avg_auc_tests, 'std_train': std_auc_train, 'std_tests': std_auc_tests}"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"dcrjsyBM00LN","executionInfo":{"status":"ok","timestamp":1699801178295,"user_tz":0,"elapsed":5,"user":{"displayName":"Shafaan Jafri","userId":"11281166051345270474"}}},"outputs":[],"source":["# Compare all PRIME-Trained Classifier Architectures\n","\n","all_arcs_results = []\n","X_train, y_train, hla_train, X_test, y_test, hla_test = getPrimeTrainData()\n","classifiers = getAllModels()\n","for model_idx in range(len(classifiers)):\n","    results = []\n","    num_iterations = 10\n","    for i in range(num_iterations):\n","        result = classifier_auc(model_idx, X_train, y_train, X_test, y_test, epochs=100, weight_decay=0.6)\n","        results.append(result)\n","        clear_output(wait=True)\n","        print('Completed architecture:', model_idx, '-', i, ' - (', round(result['auc_tests'],2),')')\n","\n","    # Extract the AUC scores from the results list\n","    auc_train_values = [result['auc_train'] for result in results]\n","    auc_test_values = [result['auc_tests'] for result in results]\n","\n","    # Calculate the mean and standard deviation of AUC scores\n","    auc_train_mean = np.mean(auc_train_values)\n","    auc_train_std = np.std(auc_train_values)\n","    auc_test_mean = np.mean(auc_test_values)\n","    auc_test_std = np.std(auc_test_values)\n","\n","    all_arcs_results.append([auc_train_mean, auc_train_std, auc_test_mean, auc_test_std])\n","\n","auc_tr, auc_te, std_tr, std_te = np.array(all_arcs_results)[:,0], np.array(all_arcs_results)[:,2], np.array(all_arcs_results)[:,1], np.array(all_arcs_results)[:,3]\n","plt.figure(figsize=(8,5))\n","plt.errorbar(range(len(classifiers)), auc_tr, yerr=std_tr, label='Train', color='red', marker='o')\n","plt.errorbar(range(len(classifiers)), auc_te, yerr=std_te, label='Test', color='blue', marker='o')\n","plt.title('PRIME-Trained Classifiers Performance')\n","plt.xlabel('Architectures')\n","plt.ylabel('AUC')\n","plt.tight_layout()\n","#plt.xticks(rotation=45, ha='right', rotation_mode='anchor')\n","plt.vlines(x=6, ymin=min(auc_te), ymax=max(auc_tr), label='Selected Deep Model', linestyle = '--', color = 'green')\n","plt.vlines(x=0, ymin=min(auc_te), ymax=max(auc_tr), label='Parsimonious Model', linestyle = '--', color = 'purple')\n","plt.legend(loc = 'center right', fontsize = 9)\n","plt.savefig('Images/PRIME-Trained Classifiers Performance', dpi=300)\n","plt.show();"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"AMXz0AZ900LP","executionInfo":{"status":"ok","timestamp":1699801187337,"user_tz":0,"elapsed":9,"user":{"displayName":"Shafaan Jafri","userId":"11281166051345270474"}}},"outputs":[],"source":["# Computing Results for Pan-Allelic Classifier\n","\n","pars_aucs, deep_aucs = [], []\n","classifiers = getAllModels()\n","\n","model_pars = copy.deepcopy(classifiers[0])\n","model_pars.load_state_dict(torch.load('Models/prtr-pars.pth'))\n","\n","# Convert the data to PyTorch tensors\n","X_train, y_train, hla_train, X_test, y_test, hla_test = getPrimeTrainData()\n","X_train_tensor = torch.nn.functional.one_hot(torch.LongTensor(convert_number(X_train)), num_classes=A).type(torch.FloatTensor)\n","y_train_tensor = torch.Tensor(y_train)\n","X_test_tensor = torch.nn.functional.one_hot(torch.LongTensor(convert_number(X_test)), num_classes=A).type(torch.FloatTensor)\n","y_test_tensor = torch.Tensor(y_test)\n","\n","# Calculate ROC curve and AUC\n","fpr_train, tpr_train, thresholds_train = sklearn.metrics.roc_curve(\n","    np.concatenate((np.zeros(len(X_train_tensor[y_train_tensor == 0])) + 0,\n","                    np.zeros(len(X_train_tensor[y_train_tensor == 1])) + 1), axis=0),\n","    np.concatenate((model_pars(X_train_tensor[y_train_tensor == 0]).detach().numpy(),\n","                    model_pars(X_train_tensor[y_train_tensor == 1]).detach().numpy()), axis=0)\n",")\n","if len(X_test) > 0:\n","    fpr_tests, tpr_tests, thresholds_tests = sklearn.metrics.roc_curve(\n","        np.concatenate((np.zeros(len(X_test_tensor[y_test_tensor == 0])) + 0,\n","                        np.zeros(len(X_test_tensor[y_test_tensor == 1])) + 1), axis=0),\n","        np.concatenate((model_pars(X_test_tensor[y_test_tensor == 0]).detach().numpy(),\n","                        model_pars(X_test_tensor[y_test_tensor == 1]).detach().numpy()), axis=0)\n","    )\n","    auc_train = sklearn.metrics.auc(fpr_train, tpr_train)\n","    auc_tests = sklearn.metrics.auc(fpr_tests, tpr_tests)\n","\n","print(round(auc_train, 3), round(auc_tests, 3))"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"xljRa-VZ00LQ","executionInfo":{"status":"ok","timestamp":1699801209224,"user_tz":0,"elapsed":312,"user":{"displayName":"Shafaan Jafri","userId":"11281166051345270474"}}},"outputs":[],"source":["# PRIME-Trained Deep vs Parsimonious Code on Test Data for Pan-Allelic Classifier\n","\n","from sklearn.exceptions import UndefinedMetricWarning\n","def warn(*args, **kwargs):\n","    pass\n","import warnings\n","warnings.warn = warn\n","\n","pars_aucs, deep_aucs, VA_pars_aucs, VA_deep_aucs = [], [], [], []\n","hla_count0, hla_count1 = [0] * len(list_hlas), [0] * len(list_hlas)\n","repeats = 100\n","peats = 10\n","progress_bar = tqdm(total= 2 * repeats * 10, desc='Progress')\n","for hla in list_hlas:\n","    classifiers = getAllModels()\n","    model_pars = copy.deepcopy(classifiers[0])\n","    model_pars.load_state_dict(torch.load('Models/prtr-pars.pth'))\n","    tests_all = []\n","    for i in range(repeats):\n","        # Convert the data to PyTorch tensors\n","        for i in range(peats):\n","            X_train, y_train, X_test, y_test= getPrimeTrainDataSplit(hla)\n","            if np.sum(y_test == 0) > 0 and np.sum(y_test == 1) > 0:\n","                break\n","#             if i == (peats-10):\n","#                 print(hla)\n","        hla_count0[list_hlas.index(hla)] += len(y_test[y_test==0])\n","        hla_count1[list_hlas.index(hla)] += len(y_test[y_test==1])\n","        X_test_tensor = torch.nn.functional.one_hot(torch.LongTensor(convert_number(X_test)), num_classes=A).type(torch.FloatTensor)\n","        y_test_tensor = torch.Tensor(y_test)\n","        # Calculate ROC curve and AUC\n","        if len(X_test) > 0:\n","            fpr_tests, tpr_tests, thresholds_tests = sklearn.metrics.roc_curve(\n","                np.concatenate((np.zeros(len(X_test_tensor[y_test_tensor == 0])) + 0,\n","                                np.zeros(len(X_test_tensor[y_test_tensor == 1])) + 1), axis=0),\n","                np.concatenate((model_pars(X_test_tensor[y_test_tensor == 0]).detach().numpy(),\n","                                model_pars(X_test_tensor[y_test_tensor == 1]).detach().numpy()), axis=0)\n","            )\n","            auc_tests = sklearn.metrics.auc(fpr_tests, tpr_tests)\n","        else:\n","            auc_tests = 0\n","        if(np.isnan(auc_tests)):\n","            auc_tests = 0\n","        tests_all.append(auc_tests)\n","        progress_bar.update(1)\n","    pars_aucs.append([np.average(auc_tests), np.std(auc_tests)])\n","\n","for hla in list_hlas:\n","    classifiers = getAllModels()\n","    model_deep = copy.deepcopy(classifiers[6])\n","    model_deep.load_state_dict(torch.load('Models/prtr-deep.pth'))\n","    tests_all = []\n","    for i in range(repeats):\n","        # Convert the data to PyTorch tensors\n","        for i in range(peats):\n","            X_train, y_train, X_test, y_test = getPrimeTrainDataSplit(hla)\n","            if np.sum(y_test == 0) > 0 and np.sum(y_test == 1) > 0:\n","                break\n","#             if i == (peats-10):\n","#                 print(hla)\n","        hla_count0[list_hlas.index(hla)] += len(y_test[y_test==0])\n","        hla_count1[list_hlas.index(hla)] += len(y_test[y_test==1])\n","        X_test_tensor = torch.nn.functional.one_hot(torch.LongTensor(convert_number(X_test)), num_classes=A).type(torch.FloatTensor)\n","        y_test_tensor = torch.Tensor(y_test)\n","        # Calculate ROC curve and AUC\n","        if len(X_test) > 0:\n","            fpr_tests, tpr_tests, thresholds_tests = sklearn.metrics.roc_curve(\n","                np.concatenate((np.zeros(len(X_test_tensor[y_test_tensor == 0])) + 0,\n","                                np.zeros(len(X_test_tensor[y_test_tensor == 1])) + 1), axis=0),\n","                np.concatenate((model_deep(X_test_tensor[y_test_tensor == 0]).detach().numpy(),\n","                                model_deep(X_test_tensor[y_test_tensor == 1]).detach().numpy()), axis=0)\n","            )\n","            auc_tests = sklearn.metrics.auc(fpr_tests, tpr_tests)\n","        else:\n","            auc_tests = 0\n","        if(np.isnan(auc_tests)):\n","            auc_tests = 0\n","        tests_all.append(auc_tests)\n","        progress_bar.update(1)\n","    deep_aucs.append([np.average(auc_tests), np.std(auc_tests)])"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"FTX69TuN00LR","executionInfo":{"status":"ok","timestamp":1699801214766,"user_tz":0,"elapsed":321,"user":{"displayName":"Shafaan Jafri","userId":"11281166051345270474"}}},"outputs":[],"source":["# PRIME-Trained Deep vs Parsimonious Code on Test Data for Pan-Allelic Classifier\n","\n","# Calculate the positions for the bars in each group\n","bar_width = 0.2\n","x_positions = np.arange(len(list_hlas))\n","fig, ax1 = plt.subplots(figsize=(10, 6))\n","\n","# Create a twin axes on the right side for the counts\n","ax2 = ax1.twinx()\n","\n","# Plot the bar charts on the right y-axis\n","ax2.bar(x_positions - bar_width/2, hla_count0, width=bar_width, alpha=0.5, color='lime', label='Count (Neg. in Test)', zorder=2)\n","ax2.bar(x_positions + bar_width/2, hla_count1, width=bar_width, alpha=0.5, color='green', label='Count (Pos. in Test)', zorder=2)\n","\n","# Set the range for the right y-axis (counts)\n","ax2.set_ylim(0, max(max(hla_count0), max(hla_count1)))\n","\n","# Label the right y-axis\n","ax2.set_ylabel('HLA Test Data Count')\n","\n","ax2.legend(loc='upper right', fontsize=9)\n","\n","# Plot the line charts for the AUC values on the left y-axis\n","ax1.errorbar(x_positions, np.array(pars_aucs)[:, 0], yerr=np.array(pars_aucs)[:,1], label='Parsimonious Test', color='orange', marker='o', capsize=5, zorder=3)\n","ax1.errorbar(x_positions, np.array(deep_aucs)[:, 0], yerr=np.array(deep_aucs)[:,1], label='Deep Test', color='cyan', marker='o', capsize=5, zorder=3)\n","\n","ax1.set_xticks(x_positions)\n","ax1.set_xticklabels(list_hlas, rotation=45, ha='right', rotation_mode='anchor')\n","ax1.set_xlabel('HLAs')\n","ax1.set_ylabel('PRIME-Trained Classifier AUC')\n","ax1.set_ylim(0, 1)\n","ax1.legend(loc='upper left', fontsize=9)\n","ax1.set_title('Pan-Allelic Classifier Performance by Allele (PRIME-Trained)')\n","\n","plt.tight_layout(pad=2)\n","plt.savefig('Images/Pan-Allelic Classifier Performance by Allele (PRIME-Trained)', dpi=300)\n","plt.show();"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"zz-UKnrH00LT","executionInfo":{"status":"ok","timestamp":1699801219258,"user_tz":0,"elapsed":199,"user":{"displayName":"Shafaan Jafri","userId":"11281166051345270474"}}},"outputs":[],"source":["# Running Single-Allele Classifier\n","\n","# Grid search was ran to find best model and split\n","repeats = 10\n","auc_all = []\n","hla_aucs_parsimonious, hla_aucs_deep, hla_stds_parsimonious, hla_stds_deep = [], [], [], []\n","\n","classifiers = getModelsBest()\n","progress_bar = tqdm(total = 2 * repeats * 10, desc='Progress')\n","\n","\n","for hla in list_hlas:\n","    for i in range(repeats):\n","#         print('Completing Parsimonious:', hla, ' - ', i)\n","        X_train, y_train, X_test, y_test = getPrimeTrainDataSplit(hla)\n","        auc = classifier_auc(0, X_train, y_train, X_test, y_test, epochs=10000)\n","        auc_all.append([auc['auc_train'], auc['auc_tests']])\n","        progress_bar.update(1)\n","    hla_aucs_parsimonious.append([np.average(np.array(auc_all)[:,0]), np.average(np.array(auc_all)[:,1])])\n","    hla_stds_parsimonious.append([np.std(np.array(auc_all)[:,0]), np.std(np.array(auc_all)[:,1])])\n","\n","    auc_all = []\n","    for i in range(repeats):\n","#         print('Completing Deep:', hla, ' - ', i)\n","        X_train, y_train, X_test, y_test = getPrimeTrainDataSplit(hla)\n","        auc = classifier_auc(1, X_train, y_train, X_test, y_test, epochs=10000)\n","        auc_all.append([auc['auc_train'], auc['auc_tests']])\n","        progress_bar.update(1)\n","    hla_aucs_deep.append([np.average(np.array(auc_all)[:,0]), np.average(np.array(auc_all)[:,1])])\n","    hla_stds_deep.append([np.std(np.array(auc_all)[:,0]), np.std(np.array(auc_all)[:,1])])\n","\n","#     print('Completed:', hla)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"-TigpGJF00LU","executionInfo":{"status":"ok","timestamp":1699801221307,"user_tz":0,"elapsed":228,"user":{"displayName":"Shafaan Jafri","userId":"11281166051345270474"}}},"outputs":[],"source":["# Single-Allele Classifier Results\n","\n","# Calculate hla_count0 and hla_count1\n","hla_count0, hla_count1 = [], []\n","for hla in list_hlas:\n","    _, tr, _, te = getPrimeTrainDataSplit(hla)\n","    hla_count0.append(len(tr[tr==0]) + len(te[te==0]))\n","    hla_count1.append(len(tr[tr==1]) + len(te[te==1]))\n","\n","# Calculate the positions for the bars in each group\n","bar_width = 0.2\n","x_positions = np.arange(len(list_hlas))\n","fig, ax1 = plt.subplots(figsize=(10, 6))\n","\n","# Create a twin axes on the right side for the counts\n","ax2 = ax1.twinx()\n","\n","# Plot the bar charts on the right y-axis\n","ax2.bar(x_positions - bar_width/2, hla_count0, width=bar_width, alpha=0.5, color='lime', label='Neg. Count', zorder=2)\n","ax2.bar(x_positions + bar_width/2, hla_count1, width=bar_width, alpha=0.5, color='green', label='Pos. Count', zorder=2)\n","ax2.set_ylim(0, max(max(hla_count0), max(hla_count1)))\n","ax2.set_ylabel('HLA Count')\n","ax2.legend(loc='upper right', fontsize=9)\n","\n","# Plot the line charts for the AUC values on the left y-axis\n","ax1.errorbar(x_positions, np.array(hla_aucs_parsimonious)[:, 0], yerr=np.array(hla_stds_parsimonious)[:, 0], label='Parsimonious Train', color='red', capsize=5, marker='o', zorder=3)\n","ax1.errorbar(x_positions, np.array(hla_aucs_parsimonious)[:, 1], yerr=np.array(hla_stds_parsimonious)[:, 1], label='Parsimonious Test', color='magenta', capsize=5, marker='o', zorder=3)\n","ax1.errorbar(x_positions, np.array(hla_aucs_deep)[:, 0], yerr=np.array(hla_stds_deep)[:, 0], label='Deep Train', color='blue', capsize=5, marker='o', zorder=3)\n","ax1.errorbar(x_positions, np.array(hla_aucs_deep)[:, 1], yerr=np.array(hla_stds_deep)[:, 1], label='Deep Test', color='purple', capsize=5, marker='o', zorder=3)\n","\n","ax1.set_xticks(x_positions)\n","ax1.set_xticklabels(list_hlas, rotation=45, ha='right', rotation_mode='anchor')\n","ax1.set_xlabel('HLAs')\n","ax1.set_ylabel('PRIME-Trained Classifier AUC')\n","ax1.set_ylim(0, 1)\n","ax1.set_title('PRIME-Trained Single Allele Classifier Performance')\n","ax1.legend(loc='lower left', fontsize=9)\n","\n","plt.tight_layout(pad=2)\n","plt.savefig('Images/PRIME-Trained Single Allele Classifier Performance', dpi=300)\n","plt.show();"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"WWccxFPa00LV","executionInfo":{"status":"ok","timestamp":1699801237384,"user_tz":0,"elapsed":203,"user":{"displayName":"Shafaan Jafri","userId":"11281166051345270474"}}},"outputs":[],"source":["# Single-Allele vs Pan-Allelic Classifier Results\n","\n","# Calculate hla_count0 and hla_count1\n","hla_count0, hla_count1 = [], []\n","for hla in list_hlas:\n","    _, _, _, te = getPrimeTrainDataSplit(hla)\n","    hla_count0.append(len(te[te==0]))\n","    hla_count1.append(len(te[te==1]))\n","\n","# Calculate the positions for the bars in each group\n","bar_width = 0.2\n","x_positions = np.arange(len(list_hlas))\n","fig, ax1 = plt.subplots(figsize=(10, 6))\n","\n","# Create a twin axes on the right side for the counts\n","ax2 = ax1.twinx()\n","\n","# Plot the bar charts on the right y-axis\n","ax2.bar(x_positions - bar_width/2, hla_count0, width=bar_width, alpha=0.5, color='lime', label='Count (Neg. in Test)', zorder=2)\n","ax2.bar(x_positions + bar_width/2, hla_count1, width=bar_width, alpha=0.5, color='green', label='Count (Pos. in Test)', zorder=2)\n","\n","# Set the range for the right y-axis (counts)\n","ax2.set_ylim(0, max(max(hla_count0), max(hla_count1)))\n","\n","# Label the right y-axis\n","ax2.set_ylabel('Counts')\n","\n","# Plot the line plots for the AUC values on the left y-axis\n","ax1.errorbar(x_positions, np.array(pars_aucs)[:, 0], yerr=np.array(pars_aucs)[:, 1], label='Pars. (Pan-Allelic)', color='orange', marker='o', capsize=5, zorder=3)\n","ax1.errorbar(x_positions, np.array(deep_aucs)[:, 0], yerr=np.array(deep_aucs)[:, 1], label='Deep. (Pan-Allelic)', color='cyan', marker='o', capsize=5, zorder=3)\n","ax1.errorbar(x_positions, np.array(hla_aucs_parsimonious)[:, 1], yerr=np.array(hla_stds_parsimonious)[:, 1], label='Pars. (Sing. Allele)', color='magenta', capsize=5, marker='o', zorder=3)\n","ax1.errorbar(x_positions, np.array(hla_aucs_deep)[:, 1], yerr=np.array(hla_stds_deep)[:, 1], label='Deep. (Sing. Allele)', color='purple', capsize=5, marker='o', zorder=3)\n","\n","ax1.set_xticks(x_positions)\n","ax1.set_xticklabels(list_hlas, rotation=45, ha='right', rotation_mode='anchor')\n","ax1.set_xlabel('HLAs')\n","ax1.set_ylabel('PRIME-Trained Classifier AUC')\n","ax1.set_title('PRIME-Trained Single vs Pan-Allelic Classifier Performances')\n","ax1.legend(fontsize=8)\n","\n","plt.tight_layout(pad=2.0)\n","plt.savefig('Images/Comparing PRIME-Trained Classifier Performances', dpi=300)\n","plt.show()"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"YH2cpir000LW","executionInfo":{"status":"ok","timestamp":1699801241374,"user_tz":0,"elapsed":199,"user":{"displayName":"Shafaan Jafri","userId":"11281166051345270474"}}},"outputs":[],"source":["# Detecting all immunogenic peptides through classifier\n","\n","# Load the FASTA file\n","fasta_path = 'EPI_ISL_402124.fasta'\n","sequences = list(DNA.read(fasta_path))\n","\n","# Concatenate all DNA sequences\n","concatenated_sequence = ''.join(str(sequence) for sequence in sequences)\n","\n","# Convert DNA sequences to amino acid protein sequences\n","protein_sequence = str(DNA(concatenated_sequence).translate())\n","\n","ref_peptides = []\n","sequence_length = len(protein_sequence)\n","peptide_length = 9\n","\n","for i in range(sequence_length - peptide_length + 1):\n","    peptide = protein_sequence[i:i + peptide_length]\n","    if '*' not in peptide and '*' not in protein_sequence[i:i + peptide_length + 1]:\n","        ref_peptides.append(peptide)\n","\n","classifiers = getModelsBest()\n","model = copy.deepcopy(classifiers[1])\n","model.load_state_dict(torch.load('Models/prtr-deep.pth')) # *Change path as required*\n","\n","ref_peptides_tensor = torch.nn.functional.one_hot(torch.LongTensor(convert_number(ref_peptides)), num_classes=A).type(torch.FloatTensor)\n","\n","ref_imm = model(ref_peptides_tensor).detach().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vTP-3Icn00LX","outputId":"99292a8e-3d0c-4fbc-e94b-50cd9d376be1"},"outputs":[{"data":{"text/plain":["263"]},"execution_count":177,"metadata":{},"output_type":"execute_result"}],"source":["# Get most immunogenic peptides\n","\n","selected_peptides = [peptide for peptide, score in zip(ref_peptides, ref_imm) if score > np.percentile(ref_imm, 95)]\n","\n","len(selected_peptides)"]}],"metadata":{"kernelspec":{"display_name":"mresenv","language":"python","name":"mresenv"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}